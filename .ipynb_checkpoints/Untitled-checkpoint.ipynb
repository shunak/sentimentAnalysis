{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-baf094941ee8>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-baf094941ee8>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    emotion analysys by RNN\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#  emotion analysis by RNN \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "with open('./reviews.txt','r') as f: # open this directory file as readonly and make referable as f\n",
    "    reviews = f.read() # f.read() substitute contents of f for reviews\n",
    "\n",
    "# print(reviews[:200]) # slice : pick up 0~199 data\n",
    "with open('./labels.txt','r') as f:\n",
    "    labels = f.read()\n",
    "\n",
    "# print(labels) \n",
    "\n",
    "from string import punctuation\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "reviews = all_text.split('\\n')\n",
    "\n",
    "all_text=' '.join(reviews) # Afrte connect all text, separate words by space.\n",
    "words = all_text.split()\n",
    "\n",
    "# print(all_text) \n",
    "\n",
    "# print(reviews)\n",
    "\n",
    "# print(words)\n",
    "from collections import Counter\n",
    "counts = Counter(words)\n",
    "vocab=sorted(counts, key=counts.get, reverse=True) #get words by desc\n",
    "\n",
    "# print(vocab)\n",
    "vocab_to_int = {word:ii for ii,word in enumerate(vocab,1)} # get key and value by enumerate\n",
    "# print(vocab_to_int) \n",
    "\n",
    "reviews_int=[] # convert reviews literal to int\n",
    "for each in reviews:\n",
    "    reviews_int.append([vocab_to_int[word] for word in each.split()])\n",
    "\n",
    "# print(reviews_int)\n",
    "# convert label to vector \n",
    "labels = labels.split('\\n')\n",
    "labels = np.array([1 if each == 'positive' else 0 for each in labels])\n",
    "\n",
    "# print(labels[:50])\n",
    "review_lens =Counter([len(x) for x in reviews_int])\n",
    "# print(review_lens[0]) \n",
    "# print(max(review_lens))\n",
    "non_zero_idx = [ii for ii, review in enumerate(reviews_int) if len(review)!=0]\n",
    "\n",
    "reviews_int = [reviews_int[ii] for ii in non_zero_idx] # get non zero value\n",
    "labels = np.array([labels[ii] for ii in non_zero_idx])\n",
    "# print(len(non_zero_idx))\n",
    "# print(reviews_int[-1])\n",
    "seq_len = 200\n",
    "features = np.zeros((len(reviews_int), seq_len),dtype=int)\n",
    "\n",
    "for i, row in enumerate(reviews_int):\n",
    "    features[i,-len(row):]=np.array(row)[:seq_len]\n",
    "\n",
    "\n",
    "# print(features[:10,:100])\n",
    "# split data for training and validation and test \n",
    "\n",
    "split_frac = 0.8\n",
    "split_idx = int(len(features)*0.8) \n",
    "\n",
    "\n",
    "train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "train_y, val_y =labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(val_x)*0.5)\n",
    "\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:] \n",
    "\n",
    "# print(\"Train set: \\t\\t{}\".format(train_x.shape))\n",
    "\n",
    "# print(\"Validation set: \\t{}\".format(val_x.shape))\n",
    "\n",
    "# print(\"Test set: \\t{}\".format(test_x.shape))\n",
    "\n",
    "\n",
    "# graph definition\n",
    "\n",
    "lstm_size = 256\n",
    "lstm_layers = 1\n",
    "batch_size = 500\n",
    "learning_rate = 0.001\n",
    "\n",
    "n_words = len(vocab_to_int) + 1\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    inputs_=tf.placeholder(tf.int32,[None,None],name='inputs')\n",
    "    labels_=tf.placeholder(tf.int32,[None,None],name='labels')\n",
    "    keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "\n",
    "embed_size=300\n",
    "\n",
    "with graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_words,embed_size),-1,1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_) # if you input embedding input-value, return word vector\n",
    "\n",
    "\n",
    "# define LSTM Cell and Layer\n",
    "\n",
    "with graph.as_default():\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm,output_keep_prob=keep_prob)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop]*lstm_layers) # results of drop*lstm_layer \n",
    "    initial_state = cell.zero_state(batch_size,tf.float32) # initalize cell\n",
    "\n",
    "    # def of output\n",
    "with graph.as_default():\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell,embed,initial_state=initial_state)\n",
    "\n",
    "# prediction\n",
    "with graph.as_default():\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:,-1],1,activation_fn=tf.sigmoid)\n",
    "    cost = tf.losses.mean_squared_error(labels_,predictions) # calc of cost function \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    # calc of learning accuracy\n",
    "\n",
    "with graph.as_default():\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions),tf.int32),labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "\n",
    "# make batch module \n",
    "def get_batches(x,y,batch_size=100):\n",
    "    n_batches = len(x)//batch_size\n",
    "    x,y = x[:n_batches*batch_size],y[:n_batches*batch_size]\n",
    "    for ii in range(0,len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size],y[ii:ii*batch_size]\n",
    "\n",
    "# training \n",
    "epochs = 10\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        for ii, (x,y) in enumerate(get_batches(train_x,train_y,batch_size),1):\n",
    "            feed ={inputs_: x,\n",
    "                    labels_:y[:,None],\n",
    "                    keep_prob:0.5,\n",
    "                    initial_state:state}\n",
    "            loss,state,_ = sess.run([cost,final_state,optimizer],feed_dict=feed)\n",
    "\n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch{}/{}\".format(e,epochs),\n",
    "                \"Iteration: []\".format(iteration),\n",
    "                \"Training Loss: {:.3f}\".format(loss))\n",
    "\n",
    "            if iteration%25==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size,tf.float32))\n",
    "                for x,y in get_batches(val_x,val_y,batch_size):\n",
    "                    feed = {inputs_ :x,\n",
    "                    labels_:y[:,None],\n",
    "                    keep_prob:1, # No Dropout\n",
    "                    initial_state: val_state}\n",
    "                \n",
    "                batch_acc, val_state = sess.run([accuracy, final_state],feed_dict=feed)\n",
    "                val_acc.append(batch_acc)\n",
    "            print(\"Value Acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "        iteration+=1\n",
    "    saver.save(sess,\"checkpoint/sentiment.ckpt\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
